diff --git a/detectron2/modeling/meta_arch/clip_rcnn.py b/detectron2/modeling/meta_arch/clip_rcnn.py
index 56524bd..2c1902b 100644
--- a/detectron2/modeling/meta_arch/clip_rcnn.py
+++ b/detectron2/modeling/meta_arch/clip_rcnn.py
@@ -1,8 +1,10 @@
 # Copyright (c) Facebook, Inc. and its affiliates.
 import logging
+import os
 import numpy as np
 from typing import Dict, List, Optional, Tuple
 from numpy.lib import pad
+import todd
 import torch
 from torch import nn
 from torch.nn import functional as F
@@ -28,6 +30,9 @@ from detectron2.utils.comm import gather_tensors, MILCrossEntropy
 
 __all__ = ["CLIPFastRCNN", "PretrainFastRCNN"]
 
+if not os.path.exists(os.environ['DUMP']):
+    os.makedirs(os.environ['DUMP'])
+
 @META_ARCH_REGISTRY.register()
 class CLIPFastRCNN(nn.Module):
     """
@@ -338,11 +343,21 @@ class CLIPFastRCNN(nn.Module):
         Rescale the output instances to the target size.
         """
         # note: private function; subject to changes
+        assert len(batched_inputs) == 1
         processed_results = []
         for results_per_image, input_per_image in zip(
             instances, batched_inputs):
             height = input_per_image["height"]  # original image size, before resizing
             width = input_per_image["width"]  # original image size, before resizing
+            torch.save(
+                dict(
+                    scores=todd.globals_.pop('scores').half(),
+                    bboxes=todd.globals_.pop('bboxes').half(),
+                    image_wh=(width, height),
+                    input_wh=results_per_image.image_size[::-1],
+                ),
+                f'{os.environ["DUMP"]}/{input_per_image["image_id"]:012d}.pth',
+            )
             r = detector_postprocess(results_per_image, height, width)
             processed_results.append({"instances": r})
         return processed_results
diff --git a/detectron2/modeling/roi_heads/fast_rcnn.py b/detectron2/modeling/roi_heads/fast_rcnn.py
index 1f46b59..902cefd 100644
--- a/detectron2/modeling/roi_heads/fast_rcnn.py
+++ b/detectron2/modeling/roi_heads/fast_rcnn.py
@@ -1,6 +1,7 @@
 # Copyright (c) Facebook, Inc. and its affiliates.
 import logging
 from typing import Dict, List, Tuple, Union
+import todd
 import torch
 from fvcore.nn import giou_loss, smooth_l1_loss
 from torch import nn
@@ -704,6 +705,9 @@ class FastRCNNOutputLayers(nn.Module):
         scores = self.predict_probs(predictions, proposals)
         image_shapes = [x.image_size for x in proposals]
 
+        todd.globals_.scores = predictions[0][:, :-1]
+        todd.globals_.bboxes = boxes[0]
+
         # optional: multiply class scores with RPN scores
         scores_bf_multiply = scores  # as a backup for visualization purpose
         if self.multiply_rpn_score and not self.training:
diff --git a/test_transfer_learning.sh b/test_transfer_learning.sh
index 635e3a5..5fd96e8 100644
--- a/test_transfer_learning.sh
+++ b/test_transfer_learning.sh
@@ -1,16 +1,16 @@
 # evaluate our trained open-vocabulary object detectors, {RN50, RN50x4} x {COCO, LVIS}
 
 # RN50, COCO (Generalized: Novel + Base)
-python3 ./tools/train_net.py \
---eval-only  \
---num-gpus 1 \
---config-file ./configs/COCO-InstanceSegmentation/CLIP_fast_rcnn_R_50_C4_ovd.yaml \
-MODEL.WEIGHTS ./pretrained_ckpt/regionclip/regionclip_finetuned-coco_rn50.pth \
-MODEL.CLIP.OFFLINE_RPN_CONFIG ./configs/COCO-InstanceSegmentation/mask_rcnn_R_50_C4_1x_ovd_FSD.yaml \
-MODEL.CLIP.BB_RPN_WEIGHTS ./pretrained_ckpt/rpn/rpn_coco_48.pth \
-MODEL.CLIP.TEXT_EMB_PATH ./pretrained_ckpt/concept_emb/coco_48_base_cls_emb.pth \
-MODEL.CLIP.OPENSET_TEST_TEXT_EMB_PATH ./pretrained_ckpt/concept_emb/coco_65_cls_emb.pth \
-MODEL.ROI_HEADS.SOFT_NMS_ENABLED True \
+# python3 ./tools/train_net.py \
+# --eval-only  \
+# --num-gpus 1 \
+# --config-file ./configs/COCO-InstanceSegmentation/CLIP_fast_rcnn_R_50_C4_ovd.yaml \
+# MODEL.WEIGHTS ./pretrained_ckpt/regionclip/regionclip_finetuned-coco_rn50.pth \
+# MODEL.CLIP.OFFLINE_RPN_CONFIG ./configs/COCO-InstanceSegmentation/mask_rcnn_R_50_C4_1x_ovd_FSD.yaml \
+# MODEL.CLIP.BB_RPN_WEIGHTS ./pretrained_ckpt/rpn/rpn_coco_48.pth \
+# MODEL.CLIP.TEXT_EMB_PATH ./pretrained_ckpt/concept_emb/coco_48_base_cls_emb.pth \
+# MODEL.CLIP.OPENSET_TEST_TEXT_EMB_PATH ./pretrained_ckpt/concept_emb/coco_65_cls_emb.pth \
+# MODEL.ROI_HEADS.SOFT_NMS_ENABLED True \
 
 # # RN50, COCO (only Novel)
 # # --config-file ./configs/COCO-InstanceSegmentation/CLIP_fast_rcnn_R_50_C4_ovd_testt.yaml \
@@ -76,4 +76,19 @@ MODEL.ROI_HEADS.SOFT_NMS_ENABLED True \
 # MODEL.ROI_BOX_HEAD.POOLER_RESOLUTION 18 \
 # MODEL.ROI_MASK_HEAD.POOLER_RESOLUTION 18 \
 # MODEL.RESNETS.RES2_OUT_CHANNELS 320 \
-# MODEL.ROI_HEADS.SOFT_NMS_ENABLED True \
\ No newline at end of file
+# MODEL.ROI_HEADS.SOFT_NMS_ENABLED True \
+
+python3 ./tools/train_net.py \
+--eval-only  \
+--num-gpus 1 \
+--config-file ./configs/COCO-InstanceSegmentation/CLIP_fast_rcnn_R_50_C4_ovd.yaml \
+MODEL.WEIGHTS ./pretrained_ckpt/regionclip/regionclip_finetuned-coco_rn50x4.pth \
+MODEL.CLIP.OFFLINE_RPN_CONFIG ./configs/COCO-InstanceSegmentation/mask_rcnn_R_50_C4_1x_ovd_FSD.yaml \
+MODEL.CLIP.BB_RPN_WEIGHTS ./pretrained_ckpt/rpn/rpn_coco_48.pth \
+MODEL.CLIP.TEXT_EMB_PATH ./pretrained_ckpt/concept_emb/coco_48_base_cls_emb_rn50x4.pth \
+MODEL.CLIP.OPENSET_TEST_TEXT_EMB_PATH ./pretrained_ckpt/concept_emb/coco_65_cls_emb_rn50x4.pth \
+MODEL.CLIP.TEXT_EMB_DIM 640 \
+MODEL.RESNETS.DEPTH 200 \
+MODEL.ROI_BOX_HEAD.POOLER_RESOLUTION 18 \
+MODEL.ROI_HEADS.SOFT_NMS_ENABLED True \
+MODEL.DEVICE cpu
